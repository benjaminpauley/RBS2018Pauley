{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting distinct places and geocoding in Python #\n",
    "\n",
    "In this script, we're going to start with the .csv file with our regularized place names that we downloaded from OpenRefine and end up with a new .csv file with the same information, but with an additional column for the latitidue and longitude coordinates.\n",
    "\n",
    "There are a few different steps here:\n",
    "* Read the .csv file line by line and get the value of the regularized_place_name cell\n",
    "* Determine whether we've already seen that place. If we have, pass it by. If we haven't, save it.\n",
    "* Use our list of distinct place names to construct a series of queries to submit via the geonames.org API\n",
    "* Interpret the results we get from geonames.org to find the latitude and longitude\n",
    "* Store the place names and corresponding latitude and longitude coordinates\n",
    "* Copy the information from our original .csv to a new .csv file, but append the appropriate latitude/longitude coordinates for each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries we'll need ##\n",
    "\n",
    "We'll use a couple of new libraries for this exercise: \n",
    "* `csv` reads and writes data from and to .csv files\n",
    "* `requests` provides easy methods for performing tasks related to getting and sending information over the internet. (The developers bill it as \"http for humans.\") The requests package isn't the only way to get these things done, but it's easy to work with\n",
    "\n",
    "We'll also import BeautifulSoup (which we used to parse XML files earlier) and json (which allows us to work with data in JSON--JavaScript Object Notation--format). In practice, we really only need one or the other of these, but I've included examples of both for purposes of illustration, since both XML and JSON are commonly used formats for data interchange. \n",
    "\n",
    "(Note: If you're using the RBSDigitalApproaches virtual machine, you'll need to make sure tha you are running this Jupyter notebook with the web-scraping virtualenvironment activated. Use `workon web-scraping` in a terminal and then run `jupyter notebook`.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a list of distinct place names ##\n",
    "\n",
    "The .csv file we downloaded from OpenRefine has regularized place names for each record. We could actually just begin geocoding straight away: reading our .csv file, getting the content of the `regularized_place_name` column and sending off a query to geonames.org. If we did that, though, we'd be sending off a *lot* of redundant requests. Our .csv file has 380 rows, but as we saw in OpenRefine, only 25 distinct regularized place names. Think of how many of those rows all read \"London UK,\" for instance. There's no need to ask Geonames the same question over and over again (\"Where is London UK?\" \"Where is London UK?\" \"Where is London UK?\") when the answer's always going to be the same. It would be like living with a four year-old. \n",
    "\n",
    "So, instead, we'll begin by reducing our 380 queries to the 25 distinct queries we really have. On the one hand, this is simply the polite thing to do: we're not hitting the Geonames server more than we need to. On the other hand, this can be in our self-interest, as well. Geonames limits the number of requests it will process from a given account per day and per hour, so reducing the number of requests we make keeps us within our limits. \n",
    "\n",
    "Now, as it happens, the limit is 30,000 requests in a day and 2,000 in an hour, so we'd be in no danger of hitting that with just 380 requests (though, actually, we *would* exceed our 2,000 hourly requests if all the people in the class all tried to make the same 380 queries simultaneously using the same geonames account). But it's still prudent to limit the number of requests--the fewer requests we make, the fewer chances there are for a response to go astray. Note, too, that, while Geonames is free, you might find yourself needing to use an API that charges, say, per 1,000 requests or something.\n",
    "\n",
    "And, anyway, we can get a distinct list of place names in just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to hold the distinct values from our regularized_place_name column\n",
    "place_names = []\n",
    "\n",
    "# Open our .csv file--which you exported from MarcEdit and saved here\n",
    "with open('/media/sf_RBSDigitalApproaches/output/Thomson_from_ESTC-full_regularized_places.csv', 'r') as infile :\n",
    "    # Hand our .csv file off to the csv module. We indicate the file we want to read; the delimiter that separates \n",
    "    # fields in the file (in this case a comma, but it could be a tab, a space, or anything else--this is an \n",
    "    # all-purpose delimited file reader); and the character that's used to set off fields that might happen\n",
    "    # to contain the delimiter (otherwise, we'd be splitting fields on every comma we encountered, which \n",
    "    # could have disastrous effects in the case of something like \"by James Thomson. With his last corrections, \n",
    "    # additions, and improvements.\"\n",
    "    #\n",
    "    # The csv module's DictReader function deals with .csv data as dictionaries rather than lists: it uses \n",
    "    # the fieldname from the first row as the key, and the content of the corresponding cell in a row as the\n",
    "    # value. This allows us to work with named fields, rather than having to keep track of indices (like\n",
    "    # remembering that the value we want is in the fifth column of the .csv and looking for row[4].)\n",
    "    reader = csv.DictReader(infile, delimiter=',',quotechar='\"')\n",
    "    \n",
    "    # Have our csv DictReader work through the .csv file one line at a time.\n",
    "    for row in reader :\n",
    "        # Get the content of the regularized_place_name column.\n",
    "        reg_place = row['regularized_place_name']\n",
    "        \n",
    "        # Check to see if that place name is in the list of distinct place names we're building.\n",
    "        if reg_place not in place_names :\n",
    "            \n",
    "            # If the place name isn't already in our list of distinct place names, add it to our list.\n",
    "            place_names.append(reg_place)\n",
    "\n",
    "# Sort our list of distinct place names in alphabetical order. (There is no real need to do this. But it was\n",
    "# driving me nuts not having it in alphabetical order when I printed it out in the next step for\n",
    "# illustration purposes...)\n",
    "place_names = sorted(place_names)\n",
    "\n",
    "# Let's have a look at the list of distinct place names we've built. Based on what we saw in OpenRefine, there\n",
    "# should be about 25 of them.\n",
    "print(place_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Geonames, interpreting the response, and storing the lat/long value (XML version) ##\n",
    "\n",
    "Now that we have a list of distinct places, we can ask Geonames for their coordinates. We'll use the requests module for building and sending off our queries to api.geonames.org. Then we'll use BeautifulSoup to parse the response (which we'll be getting in XML format) and find the latitude and longitude. Finally, we'll add the distinct place name and its corresponding coordinates to a dictionary so we can retrieve the coordinates later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to hold distinct place names as keys and lat/long coordinates as values\n",
    "geocoded = {}\n",
    "\n",
    "# Iterate through the items in our place_names list\n",
    "for place in place_names :\n",
    "    \n",
    "    # Define a list of arguments/parameters to send to Geonames, as documented at: \n",
    "    # http://www.geonames.org/export/geonames-search.html\n",
    "    # We'll be searching for our place name (requests handles the URL encoding for us), and we specify\n",
    "    # that we want only a single result (living dangerously--but I happen to know these 25 work). Finally, \n",
    "    # we supply a username. I set this one up for this course. If you are using this example later, please\n",
    "    # set up your own free account at: http://www.geonames.org/login\n",
    "    query = [('q', place), ('maxRows', 1), ('username', 'RBSDigitalApproaches')]\n",
    "    \n",
    "    # Construct and send our query to geonames with the requests module. Requests combines \n",
    "    r = requests.get('http://api.geonames.org/search', params=query)\n",
    "    print(r.url)\n",
    "    print(r.text)\n",
    "    result = r.text\n",
    "    soup = BeautifulSoup(result, 'xml')\n",
    "    print(soup('totalResultsCount')[0].string)\n",
    "    \n",
    "    if soup('totalResultsCount')[0].string != '0' :\n",
    "        lat = soup('lat')[0].string\n",
    "        lng = soup('lng')[0].string\n",
    "        latlng = lat + ',' + lng\n",
    "        geocoded.setdefault(place,'')\n",
    "        geocoded[place] = latlng\n",
    "    else :\n",
    "        geocoded[place] = 'error'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Geonames and interpreting the response (JSON version) ###\n",
    "\n",
    "This is the same query we performed in the previous cell, but shows how to use the json module to parse data you receive as JSON. I haven't added it to our geocoded dictionary, because we already have the information from our previous query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in place_names :\n",
    "    query = [('q', place), ('maxRows', 1), ('username', 'RBSDigitalApproaches')]\n",
    "    r = requests.get('http://api.geonames.org/searchJSON', params=query)\n",
    "    # Hand off the text content of the response from requests to the json module\n",
    "    jsonread = json.loads(r.text)\n",
    "    # I've included this print command just so you can see what JSON looks like--more like a Python dictionary\n",
    "    # than an XML document tree\n",
    "    print(json.dumps(jsonread, indent=4, sort_keys=False))\n",
    "    # Get the latitude and longitude, in a way analogous to what we did in XML\n",
    "    lat = jsonread['geonames'][0]['lat']\n",
    "    lng = jsonread['geonames'][0]['lng']\n",
    "    # Combine them into a latlng pair\n",
    "    latlng = lat + ',' + lng\n",
    "    print('LatLng: ' + latlng)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress check ###\n",
    "\n",
    "Let's see the contents of our dictionary of regularized place names and lat/long pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for place, coords in sorted(geocoded.items()) :\n",
    "    print(place + ': ' + coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write our results to file ###\n",
    "\n",
    "Now we'll create a new .csv file that keeps all of the bibliographic information from the .csv file we downloaded from OpenRefine and adds a new column with our lat/long pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open our two .csv files, one in read mode, the other in write mode.\n",
    "with open('/media/sf_RBSDigitalApproaches/output/Thomson_from_ESTC-full_regularized_places.csv', 'r') as infile, \\\n",
    "open('/media/sf_RBSDigitalApproaches/output/Thomson_from_ESTC_regularized_places_geocoded.csv', 'w') as outfile :\n",
    "    # Initiate our csv DictReader to read the contents of the file we started with\n",
    "    reader = csv.DictReader(infile, delimiter=',', quotechar='\"')\n",
    "    \n",
    "    # Get the fieldnames from our existing .csv\n",
    "    fieldnames = sorted(reader.next().keys())\n",
    "    # Add a new entry for 'latlng' to the list of fieldnames\n",
    "    fieldnames.append('latlng')\n",
    "    \n",
    "    # Initiate a csv DictWriter for the new .csv file we're producing\n",
    "    writer = csv.DictWriter(outfile, delimiter=',',fieldnames=fieldnames)\n",
    "    # Write the first row of fieldnames\n",
    "    writer.writeheader()\n",
    "    for row in reader :\n",
    "        # Create a new variable to hold the value that we retrieve from the geocoded dictionary, using as our \n",
    "        # key the value of the 'regularized_place_name' cell in this row of the .csv file\n",
    "        coords = geocoded[row['regularized_place_name']]\n",
    "        # Copy the current row from the .csv file in its entirety (i.e., all the bibliographic information), as\n",
    "        # a dictionary\n",
    "        result = row\n",
    "        # Create a new entry in that dictionary, with key 'latlng' (corresponding to our new fieldname from line\n",
    "        # 10) and value coords (from line 19).\n",
    "        result.setdefault('latlng',coords)\n",
    "        # Write the row to the new .csv file\n",
    "        writer.writerow(result)\n",
    "print('File complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write our results in a different way ###\n",
    "While we're at it, let's save a different .csv file that groups records together by their place of publication. This .csv can be uploaded to a map layer at Google Maps for a quick-and-dirty display of some of our geocoding work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv\n",
    "# Create an empty dictionary to hold information about our places\n",
    "data = {}\n",
    "\n",
    "# Open our .csv file of geocoded records and initiate a csv DictReader to read the fields in each row\n",
    "with open('/media/sf_RBSDigitalApproaches/output/Thomson_from_ESTC-full_regularized_places.csv', 'r') as infile :\n",
    "    reader = csv.DictReader(infile, delimiter=',', quotechar='\"')\n",
    "    \n",
    "    # Read the file one line at a time.\n",
    "    for row in reader :\n",
    "        # Get the regularized place name from the row\n",
    "        place_name = row['regularized_place_name']\n",
    "        # Use that place name as the key for an entry in the data dictionary. If that key isn't already\n",
    "        # present, create an empty entry with the place name as the key, and a dictionary as the value.\n",
    "        # That dictionary consists of two nested keys: 'latlng', which will hold the latitude and longitude\n",
    "        # cordinates; and 'content', which we start as an empty list for holding information about the books.\n",
    "        # If there's already an entry for this place name, this code will be ignored.\n",
    "        data.setdefault(\n",
    "            place_name,\n",
    "                {'latlng': '',\n",
    "                'content': []\n",
    "                }\n",
    "        )\n",
    "        # If we don't already have the latitude and longitude for this place, add it now. \n",
    "        if data[place_name]['latlng'] == '' :\n",
    "            data[place_name]['latlng'] = row['latlng']\n",
    "        # Add to the value of the 'content' key in the nested dictionary for this place name. This line\n",
    "        # concatenates a string that will include : a permalink to the ESTC, a colon and a space, \n",
    "        # the main title of the book, and its publication year in parentheses.\n",
    "        data[place_name]['content'].append(\n",
    "            \"http://estc.bl.uk/\" + row['001'] + \": \" + row['245$a'] + \" (\" + row['008$07:4'] + \")\"\n",
    "        )\n",
    "\n",
    "# Create a list of fieldnames for the .csv we're about to create\n",
    "fieldnames = ['latlng','label','content']\n",
    "\n",
    "# Create a .csv file to hold our output, initiate a csv DictWriter, and write the header row\n",
    "with open('/media/sf_RBSDigitalApproaches/output/mapping_Thomson.csv', 'w') as outfile :\n",
    "    writer = csv.DictWriter(outfile, delimiter=',', fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Get information about each of the places in our data dictionary\n",
    "    for place_name in data.keys() :\n",
    "        # Create a dictionary to hold the output we'll write to the .csv file, using our fieldnames\n",
    "        # as the keys: the latitude/longitude coordinates; the regularized place name; and an empty \n",
    "        # entry for the information about the books.\n",
    "        result = {'latlng':data[place_name]['latlng'], 'label':place_name, 'content':''}\n",
    "        # Our Google Maps labels are going to get very unwieldy if we try to put too much information\n",
    "        # in each one. First, we'll check to see if there are more than ten items in the list we called 'content'.  \n",
    "        if len(data[place_name]['content']) > 10 :\n",
    "            # If there are, we'll set the value of the 'content' key of our result dictionary to present a message\n",
    "            # saying how many results there were, and explaining that what follows are the first ten results. Note \n",
    "            # how we have to convert the result of len() to a string in order to concatenate it with our text.\n",
    "            # To present the first ten results, we use the newline character ('\\n') as the string that we'll\n",
    "            # use to join the first ten entries (i.e., 0-9) in our list of books.\n",
    "            result['content'] = \"There are \" + str(len(data[place_name]['content'])) + \\\n",
    "            \" entries. Here are the first 10:\\n\" + '\\n'.join(data[place_name]['content'][:9])\n",
    "        # If there are fewer than ten books, we'll just join them all together, separated by newline characters.\n",
    "        else :\n",
    "            result['content'] = '\\n'.join(data[place_name]['content'])\n",
    "        \n",
    "        # Let's print the result so we can see it (because dictionaries are unsorted, the fields won't come out\n",
    "        # in the order we might expect, necessarily)...\n",
    "        print(result)\n",
    "        # ... and write our result out as a line in the .csv file\n",
    "        writer.writerow(result)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
