{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping data from web sites meant for human readers #\n",
    "\n",
    "So far, we've focused on downloading from the web data that was presented with automated querying and processing in mind. But sometimes you'll have a need to get information off of the web that's presented with the assumption that a user is going to sit in front of the screen and read, perhaps clicking on links to go elsewhere or download files. In some situations, it makes more sense to automate that process so that you can assemble the data or materials you need.\n",
    "\n",
    "*Note*: Web scraping can raise ethical questions: What's okay to scrape and what's not? How much scraping is okay, and how much is abusive? People can get up to all sorts of nefarious things with web scraping. But people can also preserve and better disseminate important information by scraping it and repurposing it. You'll have to decide what is and isn't appropriate to scrape from the web.\n",
    "\n",
    "## The problem ##\n",
    "\n",
    "The British Library has made available on Flickr millions of images taken from scans of books in the public domain. They have published metadata directories of those images on GitHub and encourage anyone who wants to reuse those images as they see fit.\n",
    "\n",
    "The British Library has also made PDFs of the volumes from which those images were taken freely available in their entirety. But they have not provided metadata for those volumes to point users to them for bulk download (that I can find so far, at least). Each page for an image on Flickr, however, includes a link to the full PDF, so it's certainly possible to download scans of entire volumes from the British Library. But you have to go through Flickr first.\n",
    "\n",
    "## Our solution ##\n",
    "\n",
    "This script will search a .csv file of metadata for the British Library's Flickr image set and find images that appear in books by James Thomson (author of *Sophonisba*). We'll figure out how many distinct books those images are taken from, then get a link to one image from each book (since we only need one--they all point to the same PDF). With that link, we'll be able to download the otherwise-unindexed PDF file from the British Library's servers.\n",
    "\n",
    "## New Tricks ##\n",
    "\n",
    "Much of what we're doing in this script is just like what we've done already, but there are a few new wrinkles.\n",
    "* Filename mangling, including some some mildly gnarly regular expressions. We'll need to provide filenames for the PDFs we're downloading (the British Library's files are just arbitrary strings of characters, so we'll want something more descriptive). We'll use a few words from each title to construct a filename, but we want to avoid a bunch of pointless \"The\"s, \"A\"s, \"An\"s, etc. (i.e., what are commonly called \"stopwords\").\n",
    "* Streaming large file downloads and saving iteratively. The PDFs provided by the British Library are high quality, with correspondingly large file sizes. We don't want to wait to load the entire file into memory. We'll use the `requests` module's streaming feature to begin saving the file as we get it, rather than waiting for the whole thing to arrive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import csv\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Create an empty dictionary to hold information about books by James Thomson in the BL Flickr set\n",
    "books = {}\n",
    "# Open our .csv file of metadata, initiate a csv DictReader, and begin reading the file a line at a time\n",
    "with open('/media/sf_RBSDigitalApproaches/data/BL-Flickr-C18.csv', 'rU') as flickr_file :\n",
    "    reader = csv.DictReader(flickr_file, delimiter=',', quotechar='\"')\n",
    "    for row in reader :\n",
    "        # Check to see if the contents of the first_author cell are \"Thomson, James\"\n",
    "        if row['first_author'] == 'Thomson, James' :\n",
    "            # Save information from this row as variables\n",
    "            book_identifier = row['book_identifier']\n",
    "            flickr_url = row['flickr_url']\n",
    "            author = row['first_author']\n",
    "            title = row['title']\n",
    "            # Check to see if we already have a link for an image from this book--we only need one link per book\n",
    "            if book_identifier not in books.keys() :\n",
    "                books.setdefault(book_identifier, {'flickr_url': flickr_url, 'author': author, 'title': title})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress check ###\n",
    "Let's have a look at the information we've saved about these Flickr links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in books.items() :\n",
    "    print(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the Flickr page for the download link at the British Library ###\n",
    "\n",
    "Now that we have URLs to take us to pages at Flickr, we need to use the `requests` module to get the contents of those pages, then use `BeautifulSoup` to find the links to the PDF at the British library, which all include \"access.bl.uk\" in their URLs. We'll then add those new URLs to our `books` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book_identifier, values in books.items() :\n",
    "    # Get the flickr_url for an image from each book\n",
    "    scrape_url = values['flickr_url']\n",
    "    \n",
    "    # Use the requests module to retrieve the content at that URL\n",
    "    r = requests.get(scrape_url)\n",
    "    \n",
    "    # Take the text that requests brings back and pass it over to BeautifulSoup, using the html parser\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    # Use BeautifulSoup's find to look for a link with an href attribute that contains the pattern \"access.bl.uk,\"\n",
    "    # then get the value of the href attribute itself (i.e., just the link, not the whole tag). Save this link as\n",
    "    # 'bl_book_url'.\n",
    "    bl_book_url = soup.find(href=re.compile('access.bl.uk'))['href']\n",
    "    \n",
    "    #Use setdefault to add a new key/value pair for this new URL to our books dictionary\n",
    "    values.setdefault('bl_book_url', bl_book_url)\n",
    "    \n",
    "    # Close the request module's connection to the server\n",
    "    r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress check ###\n",
    "\n",
    "Let's see the current state of our `books` dictionary, now with links to the British Library's PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in books.items() :\n",
    "    print(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a filename for the PDF we're about to download ###\n",
    "\n",
    "We've got the link to the PDFs at the British Library, but we have to call them something when we download them: I can't tell \"lsidyv345e48ed\" from \"lsidyv345e7a47\" at a glance. We saved some basic information to our `books` dictionary, but we wouldn't want to use some of those long titles as filenames for our PDFs. Warning: things are about to get a litle weird.\n",
    "\n",
    "The filename I want to arrive at will consist of the author's last name, a few selected words of the title (eliminating any punctuation), and the British Library's identifier, all separated by underscores. So we need to:\n",
    "* Isolate the author's last name\n",
    "* Get a list of the words in the title\n",
    "* Eliminate stopwords\n",
    "* Get a manageably small number of words to use\n",
    "* Get just the identifer from the British Library's link\n",
    "* Combine all of this together with underscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book, data in books.items() :\n",
    "    # First, let's get the author's last name, using partition on the comma and taking the first item in the \n",
    "    # resulting list of strings: index[0]\n",
    "    author_name = data['author'].partition(',')[0]\n",
    "    \n",
    "    # Next, several steps for dealing with the words in the title:\n",
    "    # 1) split the title string into a list of its constituent words, splitting on whitespaces.\n",
    "    title_words = data['title'].split(' ')\n",
    "    \n",
    "    # 2) Define a regular expression to use for searching for and eliminating stop words\n",
    "    stopwords = re.compile('^\\\\[?[Aa]$|^\\\\[?[Aa]n$|^[Aa]nd$|^\\\\[?[Tt]he$|^[Oo]f$|^\\\\[?[Ww]ith$|^\\.\\.\\.$')\n",
    "    \n",
    "    # 3) Iterate through the list of title_words, searching for the stopwords regular expression pattern\n",
    "    # in each word, then removing that word from the list of title_words if we find it.\n",
    "    for word in title_words :\n",
    "        if re.search(stopwords, word) :\n",
    "            title_words.remove(word)\n",
    "    \n",
    "    # 4) Use the first four words that remain in the list, joining them together with underscores\n",
    "    prep_title = '_'.join(title_words[:4])\n",
    "    \n",
    "    # 5) Define a regular expression to look for any unwanted punctuation in our concatenated title words:\n",
    "    punct = re.compile('[\\\\[\\\\]\\.,:;?!]')\n",
    "    \n",
    "    # 6) Look for instances of that regular expression in our prep_title and, when we find one, delete it\n",
    "    # (by substituting nothing: '').\n",
    "    trunc_title = re.sub(punct,'',prep_title)\n",
    "    \n",
    "    # To get the BL identifier, we use a variation on partition--rpartition--to work from the right end of the string\n",
    "    bl_id = data['bl_book_url'].rpartition('/')[-1]\n",
    "    \n",
    "    # Put it all together to get a filename\n",
    "    filename = author_name + '_' + trunc_title + '_' + bl_id + '.pdf'\n",
    "    \n",
    "    # Add the filename to our dictionary\n",
    "    data.setdefault('filename', filename)\n",
    "    \n",
    "for book in books.items() :\n",
    "    print(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think about downloading the books ###\n",
    "\n",
    "The code in this cell will download the seven books in our set, two megabytes at a time. But the seven books together total over 300 megabytes, which is not only a lot of storage, but will also take quite a while to download. The next cell will download the smallest of the PDFs (ca. 16.1MB), though, so that you can see that it actually works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### THINK BEFORE RUNNING THIS CELL: DO YOU REALLY WANT SEVEN LARGE PDFs OF JAMES THOMSON? ######\n",
    "\n",
    "for book_identifer, values in books.items() :\n",
    "    with open('/media/sf_RBSDigitalApproaches/output/' + values['filename'], 'wb') as download :\n",
    "        r2 = requests.get(values['bl_book_url'], stream=True)\n",
    "        file_size = r2.headers['Content-Length']\n",
    "        # This is part of a very crude download timer. You can find better ideas on Stackoverflow...\n",
    "        progress = 0\n",
    "        print('Downloading ' + values['filename'] + '...')\n",
    "        for chunk in r2.iter_content(chunk_size=2000000) : \n",
    "            if chunk :\n",
    "                download.write(chunk)\n",
    "                progress += 2000000\n",
    "                print('Downloaded ' + str(progress) + ' of ' + str(file_size) + ' (' + \\\n",
    "                      str(float(progress)/float(file_size) * 100) + '%)')\n",
    "            \n",
    "        print(values['filename'] + ' complete.')\n",
    "        r2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = books['3626484']\n",
    "with open('/media/sf_RBSDigitalApproaches/output/' + sample['filename'], 'wb') as download :\n",
    "    r2 = requests.get(sample['bl_book_url'], stream=True)\n",
    "    file_size = r2.headers['Content-Length']\n",
    "    progress=0\n",
    "    print('Downloading ' + sample['filename'] + '...')\n",
    "    for chunk in r2.iter_content(chunk_size=2000000) :\n",
    "        if chunk :\n",
    "            download.write(chunk)\n",
    "            progress += 2000000\n",
    "            print('Downloaded ' + str(progress) + ' of ' + str(file_size) + \\\n",
    "                  ' (' + str(float(progress)/float(file_size) * 100) + '%)')\n",
    "    print(sample['filename'] + ' complete.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
