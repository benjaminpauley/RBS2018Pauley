{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk downloading ECCO-TCP texts #\n",
    "\n",
    "Notebook C includes code for combining information from two different .csv files. That notebook allows us to identify which texts printed by William Bowyer are present in the ECCO-TCP corpus of TEI-encoded transcriptions and to figure out the ECCO-TCP id that corresponds to each ESTC record for a work printed by Bowyer. In this notebook, we'll actually download those texts (To understand what's going on in this notebook, you may need to first review notebook C and notebook 03.)\n",
    "\n",
    "Perverse as it will seem to anybody who's ever labored over tagging a text following the TEI guidelines, we're going to throw away all of the markup to leave ourselves with a collection of plain text files. While we're at it, we'll modernize all of the long-s characters. \n",
    "\n",
    "In addition to the modules we've used before, we'll import the `os` module to create a directory in our file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Create a string variable with the beginning of the url we'll need to download our TCP texts from GitHub\n",
    "baseurl = 'https://raw.githubusercontent.com/textcreationpartnership/'\n",
    "# Create an empty list to hold the URLs we want to download\n",
    "urls = []\n",
    "# Open our .csv file with TCP ids, initiate the csv DictReader, and read the file a line at a time\n",
    "with open('/media/sf_RBSDigitalApproaches/output/Bowyer_TCP_texts.csv', 'r') as infile :\n",
    "    reader = csv.DictReader(infile, delimiter=',', quotechar='\"')\n",
    "    for row in reader :\n",
    "        # Get the TCP id\n",
    "        tcp_id = row['tcp_id']\n",
    "        # Fill in the remainder of our URL with information based on the TCP id\n",
    "        url = baseurl + tcp_id + '/master/' + tcp_id + '.xml'\n",
    "        # Add our completed URL to our list of URLs\n",
    "        urls.append(url)\n",
    "\n",
    "# Let's see what we have\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make our filenames for saving the files ###\n",
    "\n",
    "We saved the URLs, which include a filename ending in .xml, but we're going to save these files as plaintext (.txt) files. Note that, because we're outside of our earlier \"for\" loop now, we no longer have access to the tcp_id variable (which we could have simply combined with \".txt\"), so we'll have to get our filename a different way. \n",
    "\n",
    "(*Note*: If I were writing this as a freestanding script, rather than in a Jupyter Notebook, I'd probably do this differently, taking care of the URL construction, filename mangling, downloading, XML parsing, and file writing all inside that loop. That is, as I understand it, a relatively unsophisticated way to do things, but it certainly works...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls :\n",
    "    # First, use rpartition to split our url on the '/' character, starting from the right end and keeping \n",
    "    # the last bit. Then, take a portion of that string, beginning at character 0 and stopping four characters\n",
    "    # from the end of the string (this eliminates our .xml file extension). Finally, add the new file extension\n",
    "    # '.txt'.\n",
    "    filename = url.rpartition('/')[-1][0:-4] + '.txt'\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to be sure that we don't already have this folder (we'd raise an error if we tried running the code in this\n",
    "# cell a second time, because the directory would already exist)\n",
    "if not os.path.exists('/media/sf_RBSDigitalApproaches/output/Bowyer_TCP/') :\n",
    "    # If not, create the directory\n",
    "    os.mkdir('/media/sf_RBSDigitalApproaches/output/Bowyer_TCP/')\n",
    "for url in urls :\n",
    "    filename = url.rpartition('/')[-1][:-4] + '.txt'\n",
    "    # Pass the URL to the requests module and get the resource\n",
    "    r = requests.get(url)\n",
    "    # Pass the text that requests brings back over to teh BeautifulSoup module, using the xml parser from lxml. \n",
    "    soup = BeautifulSoup(r.text,'xml')\n",
    "    # Find the \"text\" element of our TEI document, then get_text() to get the text content, throwing away all \n",
    "    # the markup.\n",
    "    stripped = soup.find('text').get_text()\n",
    "    # Use the re.sub() to find all the long-s's (we have to designate that as a unicode character with the u\n",
    "    # outside the quotation marks) and replace them with short-s's.\n",
    "    modernized = re.sub(u'Å¿','s',stripped)\n",
    "    # Open a new text file in our target directory and write our modernized text to it, encoding as utf-8\n",
    "    with open('/media/sf_RBSDigitalApproaches/output/Bowyer_TCP/' + filename, 'wb') as file :\n",
    "        file.write(modernized.encode('utf-8'))\n",
    "    print('Saving ' + filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
